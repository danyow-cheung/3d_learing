{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYL0iVW07H95",
        "outputId": "b244840c-887f-4cb1-c2b8-2853960a2533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath) (2.8.2)\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt201/download.html\n",
            "Collecting pytorch3d\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt201/pytorch3d-0.7.4-cp310-cp310-linux_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (1.23.5)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (2.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (2.8.2)\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.4\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install fvcore iopath\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkn5C0wj7_Ck"
      },
      "source": [
        "A single neural network (NeRF model)\n",
        "is trained to represent a single 3D scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "omydF0Ax7283"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch3d.renderer import (FoVPerspectiveCameras,NDCMultinomialRaysampler,\n",
        "                                MonteCarloRaysampler,EmissionAbsorptionRaymarcher,\n",
        "                                ImplicitRenderer,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S7h4BfN72Gz",
        "outputId": "ae69bf2a-1c27-4cd6-c69c-ddd3e7783ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用cpu\n"
          ]
        }
      ],
      "source": [
        "from utils.helper_functions import generate_rotating_nerf,huber,sample_images_at_mc_locs\n",
        "from nerf_model import NeuralRadianceField\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "  torch.cuda.set_device(device)\n",
        "else:\n",
        "  print('使用cpu')\n",
        "  device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sTN8QcN9K6G"
      },
      "outputs": [],
      "source": [
        "# 使用幫助函數來產生訓練圖片\n",
        "from utils.plot_image_grid import image_grid\n",
        "from utils.generate_cow_renders import generate_cow_renders \n",
        "\n",
        "# 我們現在可以使用這些實用函數來產生攝影機角度、影像和輪廓\n",
        "# 從多個不同角度的合成牛。 這將列印生成的圖像的數量，\n",
        "# 輪廓和拍攝角度：\n",
        "target_cameras,target_images,target_silhouettes = generate_cow_renders(num_views=40,azimuth_range=180)\n",
        "print(f\"產生{len(target_cameras)} 張圖片\")\n",
        "\n",
        "# 正如我們在前一章中所做的那樣，\n",
        "# 讓我們定義一個光線採樣器。 我們將使用 MonteCarloRaysampler。 這會從影像平面的隨機像素子集產生光線\n",
        "render_size = target_images.shape[1] *2 \n",
        "volume_extent_world = 3.0 \n",
        "raysampler_mc = MonteCarloRaysampler(\n",
        "    min_x=-1.0,\n",
        "    max_x=1.0,\n",
        "    min_y=-1.0,\n",
        "    max_y=1.0,\n",
        "    n_rays_per_image = 750,\n",
        "    n_pts_per_ray = 128,\n",
        "    min_depth=0.1,\n",
        "    max_depth = volume_extent_world,\n",
        ")\n",
        "# 定義光線行進器\n",
        "raymarcher = EmissionAbsorptionRaymarcher()\n",
        "renderer_mc = ImplicitRenderer(raysampler=raysampler_mc,raymarcher=raymarcher)\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "。 在訓練模型時，可視化模型輸出總是有用的。 在許多其他中\n",
        "使用，如果我們看到模型輸出沒有改變，這將幫助我們修正方向\n",
        "時間。 到目前為止，我們已經使用了 MonteCarloRaysampler，它正在訓練\n",
        "模型，但是當我們想要渲染完整圖像時這不會有用，因為它是隨機採樣的\n",
        "射線。 為了查看完整影像，我們需要有系統地取樣與所有像素對應的光線\n",
        "在輸出幀中。 為了實現這一點，我們將使用 NDCMultinomialRaysampler："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render_size = target_images.shape[1]*2 \n",
        "volume_extent_world = 3.0 \n",
        "raysampler_grid = NDCMultinomialRaysampler(image_height=render_size,image_width=render_size,n_pts_per_ray=128,\n",
        "                                           min_depth=0.1,max_depth=volume_extent_world)\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid,raymarcher=raymarcher\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.helper_functions import show_full_render\n",
        "from nerf_model import  NeuralRadianceField\n",
        "neural_radiance = NeuralRadianceField()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "renderer_grid = renderer_grid.to(device)\n",
        "renderer_mc = renderer_mc.to(device)\n",
        "target_cameras = target_cameras.to(device)\n",
        "target_images = target_images.to(device)\n",
        "target_silhouettes = target_silhouettes.to(device)\n",
        "neural_radiance = neural_radiance.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = 1e-3 \n",
        "optimizer = torch.optim.Adam(neural_radiance.parameters(),lr=lr)\n",
        "batch_size = 6 \n",
        "n_iter = 3000 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_history_color,loss_history_sil = [],[]\n",
        "for iteration in range(n_iter):\n",
        "    if iteration==round(n_iter*0.75):\n",
        "        print('Decreasing lr 10-fold')\n",
        "        optimizer = torch.optim.Adam(neural_radiance.parameters(),lr=lr*0.1)\n",
        "    optimizer.zero_grad()\n",
        "    batch_idx= torch.randperm(len(target_cameras))[:batch_size]\n",
        "    batch_cameras = FoVPerspectiveCameras(\n",
        "        R = target_cameras.R[batch_idx],\n",
        "        T = target_cameras.T[batch_idx],\n",
        "        znear= target_cameras.znear[batch_idx],\n",
        "        zfar = target_cameras.zfar[batch_idx],\n",
        "        aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n",
        "        fov = target_cameras.fov[batch_idx],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    rendered_images_silhouettes ,sampled_rays = renderer_mc(\n",
        "        cameras=batch_cameras,\n",
        "        volumetric_function = neural_radiance\n",
        "    )\n",
        "    rendered_images,rendered_silhouettes = (\n",
        "        rendered_images_silhouettes.split([3,1],dim=-1)\n",
        "    )\n",
        "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
        "        target_silhouettes[batch_idx,...,None],\n",
        "        sampled_rays.xys\n",
        "    )\n",
        "\n",
        "    sil_err = huber(\n",
        "        rendered_silhouettes,\n",
        "        silhouettes_at_rays,\n",
        "    ).abs().mean()\n",
        "\n",
        "    colors_at_rays = sample_images_at_mc_locs(\n",
        "        target_images[batch_idx],\n",
        "        sampled_rays.xys\n",
        "    )\n",
        "    color_err = huber(\n",
        "        rendered_images,\n",
        "        colors_at_rays,\n",
        "    ).abs().mean()\n",
        "    loss = color_err + sil_err \n",
        "    loss_history_color.append(float(color_err))\n",
        "    loss_history_sil.append(float(sil_err))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if iteration %100==0:\n",
        "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
        "        fig = show_full_render(\n",
        "            neural_radiance,FoVPerspectiveCameras(\n",
        "                R = target_cameras.R[show_idx],\n",
        "                T = target_cameras.T[show_idx],\n",
        "                znear = target_cameras.znear[show_idx],\n",
        "                zfar = target_cameras.zfar[show_idx],\n",
        "                aspect_ratio=target_cameras.aspect_ratio[show_idx],\n",
        "                fov = target_cameras.fov[show_idx],\n",
        "                device=device\n",
        "            ),\n",
        "            target_images[show_idx][0],\n",
        "            target_silhouettes[show_idx][0],\n",
        "            renderer_grid,\n",
        "            loss_history_color,\n",
        "            loss_history_sil\n",
        "        )\n",
        "    fig.save(f'intermediate_{iteration}')\n",
        "    \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    rotating_nerf_frames = generate_rotating_nerf(\n",
        "        neural_radiance,n_frames=3*5\n",
        "    )\n",
        "image_grid(rotating_nerf_frames.clamp(0.,1.).cpu().numpy(),rows=3,cols=5,rgb=True,fill=True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
