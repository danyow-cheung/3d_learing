{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# differentiable volumetric rendering\n",
        "## Reconstructing 3D models from multi-view images\n",
        "\n",
        "在本節中，我們將展示一個使用可微體積渲染從多視圖影像重建 3D 模型的範例\n",
        "\n",
        "\n",
        "Of course, to reconstruct the 3D world, we need multiple images from multiple views.\n"
      ],
      "metadata": {
        "id": "ljNu3KsLIIuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "pyt_version = torch.__version__.split('+')[0].replace(\".\",\"\")\n",
        "version_str = \"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace('.',''),\n",
        "    f\"_pyt{pyt_version}\"\n",
        "])\n",
        "!pip install fvcore iopath\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCJBxrOAITEd",
        "outputId": "b3208edd-39e9-4ad9-b319-dbdda06cea97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.5.0)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=bd9ac7daa4e021263694e297818ed5920fc11d28632a0084ad60af8af19b5649\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=c87244bc08e23e84adaa5da1bedf880c7329ac7ebaab2233063bd8f5c2b9a00f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt201/download.html\n",
            "Collecting pytorch3d\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt201/pytorch3d-0.7.4-cp310-cp310-linux_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (1.23.5)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (2.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (2.8.2)\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the BSD-style license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def image_grid(\n",
        "    images,\n",
        "    rows=None,\n",
        "    cols=None,\n",
        "    fill: bool = True,\n",
        "    show_axes: bool = False,\n",
        "    rgb: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    A util function for plotting a grid of images.\n",
        "\n",
        "    Args:\n",
        "        images: (N, H, W, 4) array of RGBA images\n",
        "        rows: number of rows in the grid\n",
        "        cols: number of columns in the grid\n",
        "        fill: boolean indicating if the space between images should be filled\n",
        "        show_axes: boolean indicating if the axes of the plots should be visible\n",
        "        rgb: boolean, If True, only RGB channels are plotted.\n",
        "            If False, only the alpha channel is plotted.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if (rows is None) != (cols is None):\n",
        "        raise ValueError(\"Specify either both rows and cols or neither.\")\n",
        "\n",
        "    if rows is None:\n",
        "        rows = len(images)\n",
        "        cols = 1\n",
        "\n",
        "    gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0} if fill else {}\n",
        "    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))\n",
        "    bleed = 0\n",
        "    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n",
        "\n",
        "    for ax, im in zip(axarr.ravel(), images):\n",
        "        if rgb:\n",
        "            # only render RGB channels\n",
        "            ax.imshow(im[..., :3])\n",
        "        else:\n",
        "            # only render Alpha channel\n",
        "            ax.imshow(im[..., 3])\n",
        "        if not show_axes:\n",
        "            ax.set_axis_off()"
      ],
      "metadata": {
        "id": "y5JJ7TAjJrbl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mrkavZgKEkG",
        "outputId": "df746691-586d-4ff6-ffe2-52968bc31016"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the BSD-style license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch3d.io import load_objs_as_meshes\n",
        "from pytorch3d.renderer import (\n",
        "    BlendParams,\n",
        "    FoVPerspectiveCameras,\n",
        "    MeshRasterizer,\n",
        "    MeshRenderer,\n",
        "    PointLights,\n",
        "    RasterizationSettings,\n",
        "    SoftPhongShader,\n",
        "    SoftSilhouetteShader,\n",
        "    look_at_view_transform,\n",
        ")\n",
        "\n",
        "\n",
        "# create the default data directory\n",
        "# current_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "current_dir = '/content'\n",
        "DATA_DIR = os.path.join(current_dir, \".\", \"data\", \"cow_mesh\")\n",
        "\n",
        "\n",
        "def generate_cow_renders(\n",
        "    num_views: int = 40, data_dir: str = DATA_DIR, azimuth_range: float = 180\n",
        "):\n",
        "    \"\"\"\n",
        "    This function generates `num_views` renders of a cow mesh.\n",
        "    The renders are generated from viewpoints sampled at uniformly distributed\n",
        "    azimuth intervals. The elevation is kept constant so that the camera's\n",
        "    vertical position coincides with the equator.\n",
        "\n",
        "    For a more detailed explanation of this code, please refer to the\n",
        "    docs/tutorials/fit_textured_mesh.ipynb notebook.\n",
        "\n",
        "    Args:\n",
        "        num_views: The number of generated renders.\n",
        "        data_dir: The folder that contains the cow mesh files. If the cow mesh\n",
        "            files do not exist in the folder, this function will automatically\n",
        "            download them.\n",
        "\n",
        "    Returns:\n",
        "        cameras: A batch of `num_views` `FoVPerspectiveCameras` from which the\n",
        "            images are rendered.\n",
        "        images: A tensor of shape `(num_views, height, width, 3)` containing\n",
        "            the rendered images.\n",
        "        silhouettes: A tensor of shape `(num_views, height, width)` containing\n",
        "            the rendered silhouettes.\n",
        "    \"\"\"\n",
        "\n",
        "    # set the paths\n",
        "\n",
        "    # download the cow mesh if not done before\n",
        "    cow_mesh_files = [\n",
        "        os.path.join(data_dir, fl) for fl in (\"cow.obj\", \"cow.mtl\", \"cow_texture.png\")\n",
        "    ]\n",
        "    if any(not os.path.isfile(f) for f in cow_mesh_files):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        os.system(\n",
        "            f\"wget -P {data_dir} \"\n",
        "            + \"https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj\"\n",
        "        )\n",
        "        os.system(\n",
        "            f\"wget -P {data_dir} \"\n",
        "            + \"https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.mtl\"\n",
        "        )\n",
        "        os.system(\n",
        "            f\"wget -P {data_dir} \"\n",
        "            + \"https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow_texture.png\"\n",
        "        )\n",
        "\n",
        "    # Setup\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        torch.cuda.set_device(device)\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Load obj file\n",
        "    obj_filename = os.path.join(data_dir, \"cow.obj\")\n",
        "    mesh = load_objs_as_meshes([obj_filename], device=device)\n",
        "\n",
        "    # We scale normalize and center the target mesh to fit in a sphere of radius 1\n",
        "    # centered at (0,0,0). (scale, center) will be used to bring the predicted mesh\n",
        "    # to its original center and scale.  Note that normalizing the target mesh,\n",
        "    # speeds up the optimization but is not necessary!\n",
        "    verts = mesh.verts_packed()\n",
        "    N = verts.shape[0]\n",
        "    center = verts.mean(0)\n",
        "    scale = max((verts - center).abs().max(0)[0])\n",
        "    mesh.offset_verts_(-(center.expand(N, 3)))\n",
        "    mesh.scale_verts_((1.0 / float(scale)))\n",
        "\n",
        "    # Get a batch of viewing angles.\n",
        "    elev = torch.linspace(0, 0, num_views)  # keep constant\n",
        "    azim = torch.linspace(-azimuth_range, azimuth_range, num_views) + 180.0\n",
        "\n",
        "    # Place a point light in front of the object. As mentioned above, the front of\n",
        "    # the cow is facing the -z direction.\n",
        "    lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
        "\n",
        "    # Initialize an OpenGL perspective camera that represents a batch of different\n",
        "    # viewing angles. All the cameras helper methods support mixed type inputs and\n",
        "    # broadcasting. So we can view the camera from the a distance of dist=2.7, and\n",
        "    # then specify elevation and azimuth angles for each viewpoint as tensors.\n",
        "    R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
        "    cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
        "\n",
        "    # Define the settings for rasterization and shading. Here we set the output\n",
        "    # image to be of size 128X128. As we are rendering images for visualization\n",
        "    # purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to\n",
        "    # rasterize_meshes.py for explanations of these parameters.  We also leave\n",
        "    # bin_size and max_faces_per_bin to their default values of None, which sets\n",
        "    # their values using heuristics and ensures that the faster coarse-to-fine\n",
        "    # rasterization method is used.  Refer to docs/notes/renderer.md for an\n",
        "    # explanation of the difference between naive and coarse-to-fine rasterization.\n",
        "    raster_settings = RasterizationSettings(\n",
        "        image_size=128, blur_radius=0.0, faces_per_pixel=1\n",
        "    )\n",
        "\n",
        "    # Create a Phong renderer by composing a rasterizer and a shader. The textured\n",
        "    # Phong shader will interpolate the texture uv coordinates for each vertex,\n",
        "    # sample from a texture image and apply the Phong lighting model\n",
        "    blend_params = BlendParams(sigma=1e-4, gamma=1e-4, background_color=(0.0, 0.0, 0.0))\n",
        "    renderer = MeshRenderer(\n",
        "        rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "        shader=SoftPhongShader(\n",
        "            device=device, cameras=cameras, lights=lights, blend_params=blend_params\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Create a batch of meshes by repeating the cow mesh and associated textures.\n",
        "    # Meshes has a useful `extend` method which allows us do this very easily.\n",
        "    # This also extends the textures.\n",
        "    meshes = mesh.extend(num_views)\n",
        "\n",
        "    # Render the cow mesh from each viewing angle\n",
        "    target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
        "\n",
        "    # Rasterization settings for silhouette rendering\n",
        "    sigma = 1e-4\n",
        "    raster_settings_silhouette = RasterizationSettings(\n",
        "        image_size=128, blur_radius=np.log(1.0 / 1e-4 - 1.0) * sigma, faces_per_pixel=50\n",
        "    )\n",
        "\n",
        "    # Silhouette renderer\n",
        "    renderer_silhouette = MeshRenderer(\n",
        "        rasterizer=MeshRasterizer(\n",
        "            cameras=cameras, raster_settings=raster_settings_silhouette\n",
        "        ),\n",
        "        shader=SoftSilhouetteShader(),\n",
        "    )\n",
        "\n",
        "    # Render silhouette images.  The 3rd channel of the rendering output is\n",
        "    # the alpha/silhouette channel\n",
        "    silhouette_images = renderer_silhouette(meshes, cameras=cameras, lights=lights)\n",
        "\n",
        "    # binary silhouettes\n",
        "    silhouette_binary = (silhouette_images[..., 3] > 1e-4).float()\n",
        "\n",
        "    return cameras, target_images[..., :3], silhouette_binary"
      ],
      "metadata": {
        "id": "4FyAiwc4J17r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pytorch3d.structures import Volumes\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    VolumeRenderer,\n",
        "    NDCGridRaysampler,\n",
        "    EmissionAbsorptionRaymarcher\n",
        ")\n",
        "from pytorch3d.transforms import so3_exp_map\n",
        "# from plot_image_grid import image_grid  # 直接在上面\n",
        "# from generate_cow_renders import generate_cow_renders # 如上文"
      ],
      "metadata": {
        "id": "YzMsuAU4ITbW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "IkLEpNoQI80y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "產生40張相機圖片，不同角度的剪影影像。"
      ],
      "metadata": {
        "id": "ubXOx3GPKSMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_cameras,target_images ,target_silhouettes = generate_cow_renders(num_views=40)"
      ],
      "metadata": {
        "id": "qltaP4LZKZPQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_cow_renders(num_views=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbr5pCUMKOMC",
        "outputId": "652f60d1-ffdf-4b42-aedf-e09e554b6bd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(FoVPerspectiveCameras(),\n",
              " tensor([[[[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]],\n",
              " \n",
              "          [[0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.],\n",
              "           [0., 0., 0.]]]], device='cuda:0'),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下來，我們定義一個光線採樣器。 正如我們在前面幾節中討論的，光線採樣器用於採樣光線，以及每條光線的點數："
      ],
      "metadata": {
        "id": "EasucbGcK2kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "render_size = 128\n",
        "volume_extent_world = 3.0\n",
        "raysampler = NDCGridRaysampler(\n",
        "    image_width = render_size,\n",
        "    image_height=render_size,\n",
        "    n_pts_per_ray = 150,\n",
        "    min_depth = 0.1 ,\n",
        "    max_depth = volume_extent_world,\n",
        ")"
      ],
      "metadata": {
        "id": "kuBEztpgK3os"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下來，我們像以前一樣創建光線行進器。 注意，這次，我們定義了一個 VolumeRenderer 類型的變數渲染器。 VolumeRenderer 只是一個很好的介面，光線採樣器和光線行進器在後台完成所有繁重的工作："
      ],
      "metadata": {
        "id": "hmKXAQ5fLaiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raymarcher = EmissionAbsorptionRaymarcher()\n",
        "renderer = VolumeRenderer(\n",
        "    raysampler=raysampler,raymarcher=raymarcher\n",
        ")"
      ],
      "metadata": {
        "id": "2POtLL8WLbbW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下來，我們定義一個 VolumeModel 類別。 此類別僅用於封裝體積，以便可以在前向函數中計算梯度，並且最佳化器可以更新體積密度和顏色："
      ],
      "metadata": {
        "id": "JgyMEgAwLrh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VolumeModel(torch.nn.Module):\n",
        "  def __init__(self,renderer,volume_size=[64]*3,voxel_size=0.1):\n",
        "    super().__init__()\n",
        "    self.log_densities = torch.nn.Parameter(-4.0*torch.ones(1,*volume_size))\n",
        "    self.log_colors = torch.nn.Parameter(torch.zeros(3,*volume_size))\n",
        "    self._voxel_size = voxel_size\n",
        "    self._renderer = renderer\n",
        "\n",
        "  def forward(self,cameras):\n",
        "    batch_size = cameras.R.shape[0]\n",
        "    densities = torch.sigmoid(self.log_densities)\n",
        "    colors = torch.sigmoid(self.log_colors)\n",
        "    volumes = Volumes(\n",
        "        densities==densities[None].expand(\n",
        "            batch_size,*self.log_densities.shape\n",
        "        ),features=colors[None].expand(batch_size,*self.log_colors.shape),\n",
        "        voxel_size=self._voxel_size,\n",
        "    )\n",
        "    return self._renderer(cameras=cameras,volumes=volumes)[0]"
      ],
      "metadata": {
        "id": "3sNv0Ma8LqSd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "定義 Huber 損失函數。 Huber 損失函數是一種穩健的損失函數，可防止少數異常值拖累最佳化遠離真正的最佳解。 最小化此損失函數將使 x 更接近 y："
      ],
      "metadata": {
        "id": "uVBQE6o6M31E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def huber(x,y,scaling=0.1):\n",
        "  diff_sq = (x-y)**2\n",
        "  loss = ((1+diff_sq/(scaling**2)).clamp(1e-4).sqrt() -1 ) * float(scaling)\n",
        "  return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "S0SP077CM0VB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_cameras = target_cameras.to(device)\n",
        "target_images = target_images.to(device)\n",
        "target_silhouettes = target_silhouettes.to(device)\n",
        "volume_size = 128\n",
        "# volume_model = VolumeModel(\n",
        "#     renderer,\n",
        "#     volume_size=[volume_size]*3,\n",
        "#     voxel_size=volume_extent_world/volume_size,\n",
        "# ).to(device)\n",
        "\n",
        "volume_model = VolumeModel(\n",
        "    renderer,\n",
        "    volume_size=[volume_size] * 3,\n",
        "    voxel_size=volume_extent_world / volume_size,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "Z7McYKUzOs4H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "optimizer = torch.optim.Adam(volume_model.parameters(),lr=lr)\n",
        "batch_size = 10\n",
        "n_iter = 300"
      ],
      "metadata": {
        "id": "6PjcU9vdPE-7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch3d.renderer.points.pulsar.unified import FoVOrthographicCameras\n",
        "for iteration in range(n_iter):\n",
        "  if iteration == round(n_iter*0.75):\n",
        "    print('Decreasing LR 10 -fold')\n",
        "    optimizer = torch.optim.Adam(volume_model.parameters(),lr=lr*0.1)\n",
        "  optimizer.zero_grad()\n",
        "  batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
        "  # sample the minibatch of cameras\n",
        "  batch_cameras = FoVPerspectiveCameras(\n",
        "      R = target_cameras.R[batch_idx],\n",
        "      T = target_cameras.T[batch_idx],\n",
        "      znear = target_cameras.znear[batch_idx],\n",
        "      zfar = target_cameras.zfar[batch_idx],\n",
        "      aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
        "      fov = target_cameras.fov[batch_idx],\n",
        "      device=device\n",
        "  )\n",
        "  rendered_images ,rendered_silhouettes = volume_model(\n",
        "      batch_cameras\n",
        "  ).split([3,1],dim=-1)\n",
        "  # rendered_images, rendered_silhouettes = volume_model(\n",
        "  #       batch_cameras\n",
        "  #   ).split([3, 1], dim=-1)\n",
        "\n",
        "\n",
        "  sil_err = huber(rendered_silhouettes[...,0],target_silhouettes[batch_idx],).abs().mean()\n",
        "  color_err = huber(\n",
        "      rendered_images,target_images[batch_idx]\n",
        "  ).abs().mean()\n",
        "\n",
        "  loss = color_err + sil_err\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  # with torch.no_grad():\n",
        "    # rotating_volume_frames = generate_roa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "N4KLMfRWPX3R",
        "outputId": "2ab3700c-bc31-45dc-ec94-3ff6bd6a1d4e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e124f5c7f7dc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   )\n\u001b[0;32m---> 18\u001b[0;31m   rendered_images ,rendered_silhouettes = volume_model(\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mbatch_cameras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   ).split([3,1],dim=-1)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-26d26ad13aea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cameras)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mvoxel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_voxel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcameras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcameras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvolumes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolumes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch3d/renderer/implicit/renderer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cameras, volumes, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \"\"\"\n\u001b[1;32m    252\u001b[0m         \u001b[0mvolumetric_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVolumeSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         return self.renderer(\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mcameras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcameras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolumetric_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolumetric_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch3d/renderer/implicit/renderer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cameras, volumetric_function, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# ray points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# pyre-fixme[23]: Unable to unpack `object` into 2 values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         rays_densities, rays_features = volumetric_function(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mray_bundle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mray_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcameras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcameras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch3d/renderer/implicit/renderer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ray_bundle, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# run the grid sampler on the volumes densities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         rays_densities = torch.nn.functional.grid_sample(\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mvolumes_densities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mrays_points_local_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4242\u001b[0m         \u001b[0malign_corners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mode_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"grid_sampler_3d_cuda\" not implemented for 'Bool'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rotating_volume(volume_model, n_frames=50):\n",
        "    logRs = torch.zeros(n_frames, 3, device=device)\n",
        "    logRs[:, 1] = torch.linspace(0.0, 2.0 * 3.14, n_frames, device=device)\n",
        "    Rs = so3_exp_map(logRs)\n",
        "    Ts = torch.zeros(n_frames, 3, device=device)\n",
        "    Ts[:, 2] = 2.7\n",
        "    frames = []\n",
        "    print('Generating rotating volume ...')\n",
        "    for R, T in zip(Rs, Ts):\n",
        "        camera = FoVPerspectiveCameras(\n",
        "            R=R[None],\n",
        "            T=T[None],\n",
        "            znear=target_cameras.znear[0],\n",
        "            zfar=target_cameras.zfar[0],\n",
        "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
        "            fov=target_cameras.fov[0],\n",
        "            device=device,\n",
        "        )\n",
        "        frames.append(volume_model(camera)[..., :3].clamp(0.0, 1.0))\n",
        "    return torch.cat(frames)\n"
      ],
      "metadata": {
        "id": "m8tEnnj6UiwM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  rotating_volume_frames = generate_rotating_volume(volume_model,n_frames=7*4)\n",
        "image_grid(rotating_volume_frames.clamp(0.,1..cpu().numpy(),rows=4,cols=7,rgb=True,fill=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "plpv8KTWQbc_",
        "outputId": "227414f2-63bd-4afc-91d0-62da0349e5ef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2d33d4f6aa2c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mrotating_volume_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_rotating_volume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotating_volume_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_rotating_volume' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRoQ8SzhUfsd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}